{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 9076: expected 15 fields, saw 22\\nSkipping line 19256: expected 15 fields, saw 22\\nSkipping line 24313: expected 15 fields, saw 22\\nSkipping line 47211: expected 15 fields, saw 22\\nSkipping line 54295: expected 15 fields, saw 22\\nSkipping line 56641: expected 15 fields, saw 22\\nSkipping line 63067: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 93796: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 132806: expected 15 fields, saw 22\\nSkipping line 164631: expected 15 fields, saw 22\\nSkipping line 167019: expected 15 fields, saw 22\\nSkipping line 167212: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 198103: expected 15 fields, saw 22\\nSkipping line 199191: expected 15 fields, saw 22\\nSkipping line 202841: expected 15 fields, saw 22\\nSkipping line 218228: expected 15 fields, saw 22\\nSkipping line 235900: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 277761: expected 15 fields, saw 22\\nSkipping line 304582: expected 15 fields, saw 22\\nSkipping line 312029: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343692: expected 15 fields, saw 22\\nSkipping line 352291: expected 15 fields, saw 22\\nSkipping line 363414: expected 15 fields, saw 22\\nSkipping line 378087: expected 15 fields, saw 22\\nSkipping line 378720: expected 15 fields, saw 22\\nSkipping line 378760: expected 15 fields, saw 22\\nSkipping line 379336: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 402682: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 466560: expected 15 fields, saw 22\\nSkipping line 486823: expected 15 fields, saw 22\\nSkipping line 489036: expected 15 fields, saw 22\\nSkipping line 496148: expected 15 fields, saw 22\\nSkipping line 522330: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 552961: expected 15 fields, saw 22\\nSkipping line 577388: expected 15 fields, saw 22\\nSkipping line 582182: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 590653: expected 15 fields, saw 22\\nSkipping line 608846: expected 15 fields, saw 22\\nSkipping line 615442: expected 15 fields, saw 22\\nSkipping line 645607: expected 15 fields, saw 22\\nSkipping line 654323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 714935: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 749608: expected 15 fields, saw 22\\nSkipping line 753868: expected 15 fields, saw 22\\nSkipping line 762504: expected 15 fields, saw 22\\nSkipping line 771706: expected 15 fields, saw 22\\nSkipping line 773376: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 792407: expected 15 fields, saw 22\\nSkipping line 793933: expected 15 fields, saw 22\\nSkipping line 813269: expected 15 fields, saw 22\\nSkipping line 835491: expected 15 fields, saw 22\\nSkipping line 841176: expected 15 fields, saw 22\\nSkipping line 844604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 857952: expected 15 fields, saw 22\\nSkipping line 859568: expected 15 fields, saw 22\\nSkipping line 860789: expected 15 fields, saw 22\\nSkipping line 863093: expected 15 fields, saw 22\\nSkipping line 881608: expected 15 fields, saw 22\\nSkipping line 891157: expected 15 fields, saw 22\\nSkipping line 893799: expected 15 fields, saw 22\\nSkipping line 906438: expected 15 fields, saw 22\\nSkipping line 914856: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 940736: expected 15 fields, saw 22\\nSkipping line 965818: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 993840: expected 15 fields, saw 22\\nSkipping line 1019036: expected 15 fields, saw 22\\nSkipping line 1019205: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1058122: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1144887: expected 15 fields, saw 22\\nSkipping line 1147255: expected 15 fields, saw 22\\nSkipping line 1164497: expected 15 fields, saw 22\\nSkipping line 1166930: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1218319: expected 15 fields, saw 22\\nSkipping line 1232868: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1307335: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1621422: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1857720: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1935753: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1988449: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded:  28.36 s\n",
      "Full Size: 1000  reviews\n",
      "Cleaning dataframe...\n",
      "Pre-.dropna size:  (1000, 7)\n",
      "Post-.dropna size: (1000, 7)\n",
      "Clean:  28.6 s\n",
      "Dropped short reviews:  28.6 s\n",
      "Reviews remaining:  822\n",
      "Saving clean dataframe...\n",
      "Saved:  28.6 s\n",
      "# Total products:  755\n",
      "# Total products with 100-500 reviews:  0\n",
      "# Total reviews:  822\n",
      "# Total reviews w/ <5 words:  0\n",
      "28.61 s\n",
      "Tokenizing Text. Grab a coffee. This may take a while....\n",
      "Tokenized Text:  29.71 s\n",
      "Saving Data & Trained Tokenizer....\n",
      "Saved 29.83 s\n",
      "Please run vectorize.py next :)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from time import time\n",
    "\n",
    "from utils import preprocess\n",
    "from stopwords import get_stopwords\n",
    "STOPWORDS = get_stopwords()\n",
    "\n",
    "t=time()\n",
    "\n",
    "# READ IN REVIEWS\n",
    "print('Loading Dataset...')\n",
    "reviews = pd.read_csv('data/amazon_reviews_us_Electronics_v1_00.tsv', sep='\\t', error_bad_lines=False)\n",
    "reviews = reviews.iloc[:1000]\n",
    "print('Dataset Loaded: ', round(time()-t,2),'s')\n",
    "print(\"Full Size:\", reviews.shape[0], ' reviews')\n",
    "\n",
    "# DROP USELESS ROWS\n",
    "print('Cleaning dataframe...')\n",
    "E_simple = reviews[['product_id', 'product_title', 'star_rating', 'review_headline','review_body',\n",
    "                    'review_id', 'review_date']]\n",
    "\n",
    "# DROP NULLS\n",
    "print('Pre-.dropna size: ', E_simple.shape)\n",
    "E_simple.dropna(inplace=True)\n",
    "print(\"Post-.dropna size:\", E_simple.shape)\n",
    "\n",
    "# RENAME COLUMNS FOR CONVENIENCE\n",
    "E_simple.rename(columns={'star_rating':'stars'}, inplace=True)\n",
    "print('Clean: ', round(time()-t,2),'s')\n",
    "\n",
    "# DROP SHORT REVIEWS, see EDA below\n",
    "E_simple = E_simple[E_simple.review_body.apply(lambda x: len(x.split())>4)]\n",
    "print('Dropped short reviews: ', round(time()-t,2),'s')\n",
    "print('Reviews remaining: ', E_simple.shape[0])\n",
    "\n",
    "\n",
    "#SAVE CHECKPOINT\n",
    "print('Saving clean dataframe...')\n",
    "E_simple.to_json('data/Electronics_cleaned.json')\n",
    "print('Saved: ', round(time()-t,2),'s')\n",
    "\n",
    "# SOME EDA\n",
    "total_products = E_simple.product_id.unique().shape[0]\n",
    "count100_500 = E_simple.groupby('product_id').count().iloc[:,0].between(100,500).sum()\n",
    "total_reviews = E_simple.shape[0]\n",
    "short_reviews = E_simple[E_simple.review_body.apply(lambda x: len(x.split())<5)].shape[0]\n",
    "print('# Total products: ', total_products)\n",
    "print('# Total products with 100-500 reviews: ', count100_500)\n",
    "print('# Total reviews: ', total_reviews)\n",
    "print('# Total reviews w/ <5 words: ', short_reviews)\n",
    "print(round(time()-t,2),'s')\n",
    "\n",
    "# TOKENIZE REVIEWS\n",
    "print('Tokenizing Text. Grab a coffee. This may take a while....')\n",
    "processed_corpus, bigrammer, trigrammer = preprocess(E_simple.review_body, stopwords=STOPWORDS, max_gram=3)\n",
    "print('Tokenized Text: ', round(time()-t,2),'s')\n",
    "\n",
    "# SAVE TRAINED GENSIM BIGRAMMER & TRIGRAMMER MODELS\n",
    "print('Saving Data & Trained Tokenizer....')\n",
    "E_simple['tokened'] = processed_corpus\n",
    "E_simple.to_json('data/Electronics_tokenized.json')\n",
    "with open('models/bigrammer.pkl', 'wb') as f:\n",
    "    pickle.dump(bigrammer,f)\n",
    "with open('models/trigrammer.pkl', 'wb') as f:\n",
    "    pickle.dump(trigrammer,f)\n",
    "    \n",
    "# SAVE A SMALLER SET FOR PRODUCTS THAT HAVE BETWEEN 20 AND 1000 REVIEWS FOR DEMO\n",
    "review_counts = E_simple.groupby('product_id').count().iloc[:,0]\n",
    "products20_1000 = list(review_counts[review_counts.between(20,1000)].index)\n",
    "E_small = E_simple[E_simple.product_id.isin(products20_1000)]\n",
    "E_small.to_json('data/Electronics_tokened_20to1000_2.json')\n",
    "    \n",
    "print('Saved', round(time()-t,2),'s')\n",
    "print('Please run vectorize.py next :)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
